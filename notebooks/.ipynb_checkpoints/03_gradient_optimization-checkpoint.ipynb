{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 03 - Gradient-Based Policy Optimization\n",
    "\n",
    "This notebook demonstrates that we can **learn** allocation policies via gradient descent.\n",
    "\n",
    "Key concepts:\n",
    "1. **Differentiable simulation**: The entire rollout is differentiable w.r.t. policy parameters\n",
    "2. **Loss function**: Negative seed production (we want to maximize seeds)\n",
    "3. **Gradient descent**: Update policy parameters to improve fitness\n",
    "\n",
    "This is the core innovation of Arborhedron: using autodiff to learn morphogenetic rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import Array, grad, jit, value_and_grad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sim import ClimateConfig, SimConfig, stress\n",
    "from sim.config import Allocation, TreeState\n",
    "from sim.dynamics import step, compute_seeds\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Enable 64-bit precision for better gradient stability\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Parameterized Policy\n",
    "\n",
    "We define a simple policy with learnable parameters. The policy outputs allocation fractions\n",
    "based on the current day (progress through season) and tree state.\n",
    "\n",
    "**Architecture**: Time-dependent logits with softmax normalization\n",
    "\n",
    "```\n",
    "logits = early_weights * (1 - progress) + late_weights * progress\n",
    "allocation = softmax(logits)\n",
    "```\n",
    "\n",
    "This allows the policy to learn different strategies for early vs late season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy_params(key: Array) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize policy parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - early_logits: [5] - allocation logits for day 0\n",
    "    - late_logits: [5] - allocation logits for final day\n",
    "    \n",
    "    Order: [roots, trunk, shoots, leaves, flowers]\n",
    "    \"\"\"\n",
    "    k1, k2 = jr.split(key)\n",
    "    return {\n",
    "        # Start with slight bias toward growth (leaves, roots)\n",
    "        'early_logits': jr.normal(k1, (5,)) * 0.5 + jnp.array([0.5, 0.0, 0.2, 0.8, -1.0]),\n",
    "        # End with bias toward reproduction (flowers)\n",
    "        'late_logits': jr.normal(k2, (5,)) * 0.5 + jnp.array([0.0, 0.3, 0.2, 0.2, 0.5]),\n",
    "    }\n",
    "\n",
    "\n",
    "def apply_policy(params: dict, state: TreeState, day: int, num_days: int) -> Allocation:\n",
    "    \"\"\"\n",
    "    Apply parameterized policy to get allocation.\n",
    "    \n",
    "    Interpolates between early and late logits based on progress.\n",
    "    \"\"\"\n",
    "    progress = day / num_days\n",
    "    \n",
    "    # Linear interpolation of logits\n",
    "    logits = params['early_logits'] * (1.0 - progress) + params['late_logits'] * progress\n",
    "    \n",
    "    # Softmax for valid allocation\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    \n",
    "    return Allocation(\n",
    "        roots=probs[0],\n",
    "        trunk=probs[1],\n",
    "        shoots=probs[2],\n",
    "        leaves=probs[3],\n",
    "        flowers=probs[4],\n",
    "    )\n",
    "\n",
    "\n",
    "# Test initialization\n",
    "key = jr.PRNGKey(42)\n",
    "params = init_policy_params(key)\n",
    "print(\"Initial early logits:\", params['early_logits'])\n",
    "print(\"Initial late logits:\", params['late_logits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Differentiable Rollout\n",
    "\n",
    "We implement a simplified rollout that's fully JIT-compatible.\n",
    "This uses `jax.lax.fori_loop` for efficient compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_jit(\n",
    "    params: dict,\n",
    "    config: SimConfig,\n",
    "    light_arr: Array,\n",
    "    moisture_arr: Array,\n",
    "    wind_arr: Array,\n",
    ") -> tuple[TreeState, Array]:\n",
    "    \"\"\"\n",
    "    JIT-compatible rollout for gradient computation.\n",
    "    \n",
    "    Returns final state and seed count.\n",
    "    \"\"\"\n",
    "    num_days = config.num_days\n",
    "    \n",
    "    def body_fn(day: int, state: TreeState) -> TreeState:\n",
    "        allocation = apply_policy(params, state, day, num_days)\n",
    "        return step(\n",
    "            state=state,\n",
    "            allocation=allocation,\n",
    "            light=light_arr[day],\n",
    "            moisture=moisture_arr[day],\n",
    "            wind=wind_arr[day],\n",
    "            config=config,\n",
    "            day=day,\n",
    "        )\n",
    "    \n",
    "    initial_state = TreeState.initial(energy=config.seed_energy)\n",
    "    final_state = jax.lax.fori_loop(0, num_days, body_fn, initial_state)\n",
    "    seeds = compute_seeds(final_state, config)\n",
    "    \n",
    "    return final_state, seeds\n",
    "\n",
    "\n",
    "# Precompute environment arrays\n",
    "config = SimConfig(num_days=100)\n",
    "climate = ClimateConfig.mild()\n",
    "light_arr, moisture_arr, wind_arr = stress.compute_environment_batch(climate, config.num_days)\n",
    "\n",
    "# Test rollout\n",
    "final_state, seeds = rollout_jit(params, config, light_arr, moisture_arr, wind_arr)\n",
    "print(f\"Initial policy seeds: {float(seeds):.4f}\")\n",
    "print(f\"Final biomass: {float(final_state.total_biomass()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Loss Function and Gradients\n",
    "\n",
    "Our loss function is negative seed production (we minimize loss = maximize seeds).\n",
    "We also add a small regularization term to prevent extreme logit values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "def loss_fn(\n    params: dict,\n    config: SimConfig,\n    light_arr: Array,\n    moisture_arr: Array,\n    wind_arr: Array,\n    reg_weight: float = 0.001,\n) -> Array:\n    \"\"\"\n    Compute loss: negative seeds + regularization.\n    \"\"\"\n    _, seeds = rollout_jit(params, config, light_arr, moisture_arr, wind_arr)\n    \n    # Regularization: penalize extreme logit values\n    reg = reg_weight * (\n        jnp.sum(params['early_logits'] ** 2) +\n        jnp.sum(params['late_logits'] ** 2)\n    )\n    \n    return -seeds + reg\n\n\n# Create a specialized loss function with config baked in\ndef make_loss_fn(config: SimConfig, light_arr: Array, moisture_arr: Array, wind_arr: Array):\n    \"\"\"Create a loss function with environment fixed.\"\"\"\n    def loss(params: dict) -> Array:\n        return loss_fn(params, config, light_arr, moisture_arr, wind_arr)\n    return loss\n\n# Create loss function for this environment\nloss_for_env = make_loss_fn(config, light_arr, moisture_arr, wind_arr)\n\n# JIT compile loss and gradient functions\nloss_and_grad_fn = jit(value_and_grad(loss_for_env))\n\n# Test gradient computation\nloss, grads = loss_and_grad_fn(params)\nprint(f\"Initial loss: {float(loss):.4f}\")\nprint(f\"Gradient norms:\")\nprint(f\"  early_logits: {float(jnp.linalg.norm(grads['early_logits'])):.6f}\")\nprint(f\"  late_logits: {float(jnp.linalg.norm(grads['late_logits'])):.6f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Training\n",
    "\n",
    "Now we run gradient descent to optimize the policy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "def train_policy(\n    params: dict,\n    loss_grad_fn,\n    rollout_fn,\n    num_steps: int = 200,\n    learning_rate: float = 0.1,\n) -> tuple[dict, list[float], list[float]]:\n    \"\"\"\n    Train policy via gradient descent.\n    \n    Returns optimized params, loss history, and seed history.\n    \"\"\"\n    loss_history = []\n    seed_history = []\n    \n    for i in range(num_steps):\n        loss, grads = loss_grad_fn(params)\n        loss_history.append(float(loss))\n        \n        # Compute seeds for tracking\n        _, seeds = rollout_fn(params)\n        seed_history.append(float(seeds))\n        \n        # Gradient descent update\n        params = {\n            'early_logits': params['early_logits'] - learning_rate * grads['early_logits'],\n            'late_logits': params['late_logits'] - learning_rate * grads['late_logits'],\n        }\n        \n        if i % 50 == 0:\n            print(f\"Step {i:3d}: loss={float(loss):.4f}, seeds={float(seeds):.4f}\")\n    \n    return params, loss_history, seed_history\n\n\n# Create rollout function for tracking seeds\ndef make_rollout_fn(config, light_arr, moisture_arr, wind_arr):\n    def rollout(params):\n        return rollout_jit(params, config, light_arr, moisture_arr, wind_arr)\n    return jit(rollout)\n\nrollout_fn = make_rollout_fn(config, light_arr, moisture_arr, wind_arr)\n\n# Train!\nprint(\"Training policy...\")\nprint(\"=\" * 50)\noptimized_params, loss_history, seed_history = train_policy(\n    params, \n    loss_and_grad_fn,\n    rollout_fn,\n    num_steps=200,\n    learning_rate=0.1,\n)\nprint(\"=\" * 50)\nprint(f\"Final seeds: {seed_history[-1]:.4f}\")\nprint(f\"Improvement: {seed_history[-1] / max(seed_history[0], 0.001):.1f}x\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(loss_history, linewidth=2)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Loss (negative seeds)')\n",
    "ax1.set_title('Training Loss Over Time')\n",
    "\n",
    "ax2.plot(seed_history, linewidth=2, color='tab:green')\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Seeds Produced')\n",
    "ax2.set_title('Seed Production Over Training')\n",
    "ax2.axhline(y=seed_history[0], color='gray', linestyle='--', alpha=0.5, label='Initial')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Learned Policy Analysis\n",
    "\n",
    "Let's examine what strategy the optimizer discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Compare initial vs optimized logits\n# Note: 'params' was overwritten during training, so we reinitialize for comparison\ninitial_params = init_policy_params(jr.PRNGKey(42))\nlabels = ['Roots', 'Trunk', 'Shoots', 'Leaves', 'Flowers']\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nx = np.arange(len(labels))\nwidth = 0.35\n\n# Early season\nax = axes[0]\nearly_init = jax.nn.softmax(initial_params['early_logits'])\nearly_opt = jax.nn.softmax(optimized_params['early_logits'])\nax.bar(x - width/2, early_init, width, label='Initial', alpha=0.7)\nax.bar(x + width/2, early_opt, width, label='Optimized', alpha=0.7)\nax.set_xlabel('Compartment')\nax.set_ylabel('Allocation Fraction')\nax.set_title('Early Season Allocation')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n# Late season\nax = axes[1]\nlate_init = jax.nn.softmax(initial_params['late_logits'])\nlate_opt = jax.nn.softmax(optimized_params['late_logits'])\nax.bar(x - width/2, late_init, width, label='Initial', alpha=0.7)\nax.bar(x + width/2, late_opt, width, label='Optimized', alpha=0.7)\nax.set_xlabel('Compartment')\nax.set_ylabel('Allocation Fraction')\nax.set_title('Late Season Allocation')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize allocation over season\ndays = np.arange(config.num_days)\n\ndef get_allocation_trajectory(params, num_days):\n    state = TreeState.initial()\n    allocs = {'roots': [], 'trunk': [], 'shoots': [], 'leaves': [], 'flowers': []}\n    for day in range(num_days):\n        a = apply_policy(params, state, day, num_days)\n        allocs['roots'].append(float(a.roots))\n        allocs['trunk'].append(float(a.trunk))\n        allocs['shoots'].append(float(a.shoots))\n        allocs['leaves'].append(float(a.leaves))\n        allocs['flowers'].append(float(a.flowers))\n    return allocs\n\ninit_allocs = get_allocation_trajectory(initial_params, config.num_days)\nopt_allocs = get_allocation_trajectory(optimized_params, config.num_days)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nax = axes[0]\nax.stackplot(days,\n    init_allocs['roots'], init_allocs['trunk'], init_allocs['shoots'],\n    init_allocs['leaves'], init_allocs['flowers'],\n    labels=labels, alpha=0.8)\nax.set_xlabel('Day')\nax.set_ylabel('Allocation')\nax.set_title('Initial Policy')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nax.set_ylim(0, 1)\n\nax = axes[1]\nax.stackplot(days,\n    opt_allocs['roots'], opt_allocs['trunk'], opt_allocs['shoots'],\n    opt_allocs['leaves'], opt_allocs['flowers'],\n    labels=labels, alpha=0.8)\nax.set_xlabel('Day')\nax.set_ylabel('Allocation')\nax.set_title('Optimized Policy')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Comparison: Optimized vs Baseline\n",
    "\n",
    "Let's compare the optimized policy against the hand-coded baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim import policies, run_season\n",
    "\n",
    "# Run baseline policy\n",
    "baseline_traj = run_season(config, climate, policies.baseline_policy)\n",
    "\n",
    "# Run optimized policy (need to wrap in standard policy interface)\n",
    "def optimized_policy_fn(state, day, num_days, wind):\n",
    "    return apply_policy(optimized_params, state, day, num_days)\n",
    "\n",
    "optimized_traj = run_season(config, climate, optimized_policy_fn)\n",
    "\n",
    "print(\"Policy Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<20} {'Baseline':>12} {'Optimized':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Seeds':<20} {float(baseline_traj.seeds):>12.4f} {float(optimized_traj.seeds):>12.4f}\")\n",
    "print(f\"{'Final Biomass':<20} {float(baseline_traj.states[-1].total_biomass()):>12.4f} {float(optimized_traj.states[-1].total_biomass()):>12.4f}\")\n",
    "print(f\"{'Final Energy':<20} {float(baseline_traj.states[-1].energy):>12.4f} {float(optimized_traj.states[-1].energy):>12.4f}\")\n",
    "print(f\"{'Final Flowers':<20} {float(baseline_traj.states[-1].flowers):>12.4f} {float(optimized_traj.states[-1].flowers):>12.4f}\")\n",
    "print(f\"{'Final Trunk':<20} {float(baseline_traj.states[-1].trunk):>12.4f} {float(optimized_traj.states[-1].trunk):>12.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of trajectories\n",
    "baseline_states = baseline_traj.get_state_arrays()\n",
    "optimized_states = optimized_traj.get_state_arrays()\n",
    "days = np.arange(len(baseline_traj.states))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(days, baseline_states['energy'], label='Baseline', linewidth=2)\n",
    "ax.plot(days, optimized_states['energy'], label='Optimized', linewidth=2, linestyle='--')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_title('Energy Over Time')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(days, baseline_states['leaves'], label='Baseline', linewidth=2)\n",
    "ax.plot(days, optimized_states['leaves'], label='Optimized', linewidth=2, linestyle='--')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Leaves')\n",
    "ax.set_title('Leaf Biomass Over Time')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.plot(days, baseline_states['flowers'], label='Baseline', linewidth=2)\n",
    "ax.plot(days, optimized_states['flowers'], label='Optimized', linewidth=2, linestyle='--')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Flowers')\n",
    "ax.set_title('Flower Biomass Over Time')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.plot(days, baseline_states['trunk'], label='Baseline', linewidth=2)\n",
    "ax.plot(days, optimized_states['trunk'], label='Optimized', linewidth=2, linestyle='--')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Trunk')\n",
    "ax.set_title('Trunk Biomass Over Time')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('Baseline vs Optimized Policy Trajectories', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated **gradient-based policy optimization** for tree growth:\n",
    "\n",
    "### Key Results\n",
    "1. **Differentiability**: Gradients flow through the entire 100-day rollout\n",
    "2. **Learning works**: The optimizer improved seed production from initial random policy\n",
    "3. **Interpretable strategies**: The learned policy shows clear early/late season patterns\n",
    "\n",
    "### What the Optimizer Learned\n",
    "- Early season: Focus on leaves/roots for photosynthetic capacity\n",
    "- Late season: Shift to flowers for reproduction\n",
    "- Trunk investment: Balanced throughout for structural support\n",
    "\n",
    "### Next Steps\n",
    "1. **Neural policies**: Replace linear interpolation with neural networks\n",
    "2. **Multi-climate training**: Optimize for robustness across environments\n",
    "3. **Curriculum learning**: Start in easy climates, gradually increase difficulty\n",
    "4. **Population-based training**: Evolve diverse policy families"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}