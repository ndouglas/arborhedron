{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 04 - Neural Policy Learning\n",
    "\n",
    "This notebook trains a neural network policy for tree growth allocation.\n",
    "\n",
    "**Key differences from 03A (linear interpolation policy):**\n",
    "1. **State-aware**: The policy sees current tree state (energy, biomass, etc.)\n",
    "2. **Environment-aware**: The policy sees current light, moisture, wind\n",
    "3. **Non-linear**: MLP can learn complex temporal/state-dependent strategies\n",
    "\n",
    "**What we expect to discover:**\n",
    "- Early season: leaves/roots focus (build photosynthetic capacity)\n",
    "- Mid season: trunk investment (before flowering)\n",
    "- Late season: flower focus (protected reproduction)\n",
    "- State-dependent adaptations (e.g., more trunk when energy is high)\n",
    "- Environment response (e.g., more trunk when wind is high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import Array, grad, jit, value_and_grad\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sim import ClimateConfig, SimConfig, NeuralPolicy, run_season, stress\n",
    "from sim.config import Allocation, TreeState\n",
    "from sim.dynamics import step, compute_seeds\n",
    "from sim.policies import (\n",
    "    baseline_policy,\n",
    "    make_policy_features,\n",
    "    apply_neural_policy,\n",
    "    softmax_allocation,\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Enable 64-bit precision for better gradient stability\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation config\n",
    "config = SimConfig(num_days=100)\n",
    "\n",
    "# Climate configs for training\n",
    "mild_climate = ClimateConfig.mild()\n",
    "windy_climate = ClimateConfig.windy()\n",
    "droughty_climate = ClimateConfig.droughty()\n",
    "\n",
    "# Precompute environment arrays for each climate\n",
    "def precompute_environment(climate, num_days):\n",
    "    \"\"\"Precompute environment arrays for JIT-compatible rollout.\"\"\"\n",
    "    light, moisture, wind = stress.compute_environment_batch(climate, num_days)\n",
    "    return light, moisture, wind\n",
    "\n",
    "mild_env = precompute_environment(mild_climate, config.num_days)\n",
    "windy_env = precompute_environment(windy_climate, config.num_days)\n",
    "droughty_env = precompute_environment(droughty_climate, config.num_days)\n",
    "\n",
    "print(f\"Environment arrays shape: {mild_env[0].shape}\")\n",
    "print(f\"Mild - mean light: {float(mild_env[0].mean()):.2f}, mean wind: {float(mild_env[2].mean()):.2f}\")\n",
    "print(f\"Windy - mean light: {float(windy_env[0].mean()):.2f}, mean wind: {float(windy_env[2].mean()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Neural Policy Architecture\n",
    "\n",
    "The `NeuralPolicy` class is defined in `sim/policies.py`. It's a simple MLP:\n",
    "- **Input (12)**: state (8) + progress (1) + environment (3)\n",
    "- **Hidden**: 2 layers × 32 units with tanh activation\n",
    "- **Output (5)**: allocation logits → softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neural policy\n",
    "key = jr.PRNGKey(42)\n",
    "policy = NeuralPolicy(key, hidden_size=32, num_hidden=2)\n",
    "\n",
    "# Count parameters\n",
    "def count_params(model):\n",
    "    \"\"\"Count total trainable parameters.\"\"\"\n",
    "    params = eqx.filter(model, eqx.is_array)\n",
    "    return sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "num_params = count_params(policy)\n",
    "print(f\"Neural policy parameters: {num_params}\")\n",
    "print(f\"Architecture: 12 → 32 → 32 → 5\")\n",
    "\n",
    "# Test forward pass\n",
    "test_state = TreeState.initial()\n",
    "test_features = make_policy_features(test_state, day=50, num_days=100, light=0.7, moisture=0.5, wind=0.3)\n",
    "test_logits = policy(test_features)\n",
    "test_alloc = softmax_allocation(test_logits)\n",
    "\n",
    "print(f\"\\nTest allocation at day 50:\")\n",
    "print(f\"  Roots: {float(test_alloc.roots):.3f}\")\n",
    "print(f\"  Trunk: {float(test_alloc.trunk):.3f}\")\n",
    "print(f\"  Shoots: {float(test_alloc.shoots):.3f}\")\n",
    "print(f\"  Leaves: {float(test_alloc.leaves):.3f}\")\n",
    "print(f\"  Flowers: {float(test_alloc.flowers):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. JIT-Compatible Rollout\n",
    "\n",
    "For efficient gradient computation, we need a fully JAX-traced rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_neural(\n",
    "    policy: NeuralPolicy,\n",
    "    config: SimConfig,\n",
    "    light_arr: Array,\n",
    "    moisture_arr: Array,\n",
    "    wind_arr: Array,\n",
    ") -> tuple[TreeState, Array]:\n",
    "    \"\"\"\n",
    "    JIT-compatible rollout with neural policy.\n",
    "    \n",
    "    Returns final state and seed count.\n",
    "    \"\"\"\n",
    "    num_days = config.num_days\n",
    "    \n",
    "    def body_fn(day: int, state: TreeState) -> TreeState:\n",
    "        # Get environment for this day\n",
    "        light = light_arr[day]\n",
    "        moisture = moisture_arr[day]\n",
    "        wind = wind_arr[day]\n",
    "        \n",
    "        # Get allocation from neural policy\n",
    "        features = make_policy_features(state, day, num_days, light, moisture, wind)\n",
    "        logits = policy(features)\n",
    "        allocation = softmax_allocation(logits)\n",
    "        \n",
    "        # Step dynamics\n",
    "        return step(\n",
    "            state=state,\n",
    "            allocation=allocation,\n",
    "            light=light,\n",
    "            moisture=moisture,\n",
    "            wind=wind,\n",
    "            config=config,\n",
    "            day=day,\n",
    "        )\n",
    "    \n",
    "    initial_state = TreeState.initial(energy=config.seed_energy)\n",
    "    final_state = jax.lax.fori_loop(0, num_days, body_fn, initial_state)\n",
    "    seeds = compute_seeds(final_state, config)\n",
    "    \n",
    "    return final_state, seeds\n",
    "\n",
    "\n",
    "# Test rollout\n",
    "final_state, seeds = rollout_neural(policy, config, *mild_env)\n",
    "print(f\"Initial policy seeds (mild): {float(seeds):.4f}\")\n",
    "print(f\"Final biomass: {float(final_state.total_biomass()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Loss Function and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    policy: NeuralPolicy,\n",
    "    config: SimConfig,\n",
    "    light_arr: Array,\n",
    "    moisture_arr: Array,\n",
    "    wind_arr: Array,\n",
    "    l2_weight: float = 0.0001,\n",
    ") -> Array:\n",
    "    \"\"\"\n",
    "    Compute loss: negative seeds + L2 regularization.\n",
    "    \n",
    "    We minimize loss = maximize seeds.\n",
    "    \"\"\"\n",
    "    _, seeds = rollout_neural(policy, config, light_arr, moisture_arr, wind_arr)\n",
    "    \n",
    "    # L2 regularization on weights (not too strong - we want the policy to commit)\n",
    "    params = eqx.filter(policy, eqx.is_array)\n",
    "    l2 = sum(jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params))\n",
    "    \n",
    "    return -seeds + l2_weight * l2\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(\n",
    "    policy: NeuralPolicy,\n",
    "    opt_state,\n",
    "    optimizer,\n",
    "    config: SimConfig,\n",
    "    light_arr: Array,\n",
    "    moisture_arr: Array,\n",
    "    wind_arr: Array,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single training step: compute loss, gradients, update.\n",
    "    \"\"\"\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_fn)(\n",
    "        policy, config, light_arr, moisture_arr, wind_arr\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, policy)\n",
    "    policy = eqx.apply_updates(policy, updates)\n",
    "    return policy, opt_state, loss\n",
    "\n",
    "\n",
    "# Test gradient computation\n",
    "loss = loss_fn(policy, config, *mild_env)\n",
    "print(f\"Initial loss: {float(loss):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Training Loop (Single Climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(\n",
    "    policy: NeuralPolicy,\n",
    "    config: SimConfig,\n",
    "    light_arr: Array,\n",
    "    moisture_arr: Array,\n",
    "    wind_arr: Array,\n",
    "    num_steps: int = 300,\n",
    "    learning_rate: float = 0.01,\n",
    ") -> tuple[NeuralPolicy, list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Train neural policy via gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "        - Trained policy\n",
    "        - Loss history\n",
    "        - Seed history\n",
    "    \"\"\"\n",
    "    # Optimizer: Adam with learning rate schedule\n",
    "    schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=learning_rate * 0.1,\n",
    "        peak_value=learning_rate,\n",
    "        warmup_steps=20,\n",
    "        decay_steps=num_steps,\n",
    "        end_value=learning_rate * 0.01,\n",
    "    )\n",
    "    optimizer = optax.adam(schedule)\n",
    "    opt_state = optimizer.init(eqx.filter(policy, eqx.is_array))\n",
    "    \n",
    "    loss_history = []\n",
    "    seed_history = []\n",
    "    \n",
    "    for i in tqdm(range(num_steps), desc=\"Training\"):\n",
    "        policy, opt_state, loss = train_step(\n",
    "            policy, opt_state, optimizer, config,\n",
    "            light_arr, moisture_arr, wind_arr\n",
    "        )\n",
    "        \n",
    "        loss_history.append(float(loss))\n",
    "        \n",
    "        # Track seeds (without regularization)\n",
    "        _, seeds = rollout_neural(policy, config, light_arr, moisture_arr, wind_arr)\n",
    "        seed_history.append(float(seeds))\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Step {i:3d}: loss={float(loss):.4f}, seeds={float(seeds):.4f}\")\n",
    "    \n",
    "    return policy, loss_history, seed_history\n",
    "\n",
    "\n",
    "# Train on mild climate\n",
    "print(\"=\"*60)\n",
    "print(\"Training on MILD climate\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "key = jr.PRNGKey(42)\n",
    "policy_mild = NeuralPolicy(key, hidden_size=32, num_hidden=2)\n",
    "\n",
    "policy_mild, loss_hist_mild, seed_hist_mild = train_policy(\n",
    "    policy_mild, config, *mild_env,\n",
    "    num_steps=300,\n",
    "    learning_rate=0.01,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal seeds: {seed_hist_mild[-1]:.4f}\")\n",
    "print(f\"Improvement: {seed_hist_mild[-1] / max(seed_hist_mild[0], 0.001):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(loss_hist_mild, linewidth=2)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Loss (negative seeds + reg)')\n",
    "ax1.set_title('Training Loss')\n",
    "\n",
    "ax2.plot(seed_hist_mild, linewidth=2, color='tab:green')\n",
    "ax2.axhline(y=seed_hist_mild[0], color='gray', linestyle='--', alpha=0.5, label='Initial')\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Seeds Produced')\n",
    "ax2.set_title('Seed Production Over Training')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Policy Analysis: What Did It Learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_policy_trajectory(policy, config, light_arr, moisture_arr, wind_arr):\n",
    "    \"\"\"\n",
    "    Run policy and collect allocation decisions at each step.\n",
    "    \"\"\"\n",
    "    num_days = config.num_days\n",
    "    state = TreeState.initial(energy=config.seed_energy)\n",
    "    \n",
    "    allocations = {k: [] for k in ['roots', 'trunk', 'shoots', 'leaves', 'flowers']}\n",
    "    states = {k: [] for k in ['energy', 'water', 'roots', 'trunk', 'shoots', 'leaves', 'flowers']}\n",
    "    \n",
    "    for day in range(num_days):\n",
    "        light = float(light_arr[day])\n",
    "        moisture = float(moisture_arr[day])\n",
    "        wind = float(wind_arr[day])\n",
    "        \n",
    "        # Record state\n",
    "        states['energy'].append(float(state.energy))\n",
    "        states['water'].append(float(state.water))\n",
    "        states['roots'].append(float(state.roots))\n",
    "        states['trunk'].append(float(state.trunk))\n",
    "        states['shoots'].append(float(state.shoots))\n",
    "        states['leaves'].append(float(state.leaves))\n",
    "        states['flowers'].append(float(state.flowers))\n",
    "        \n",
    "        # Get allocation\n",
    "        features = make_policy_features(state, day, num_days, light, moisture, wind)\n",
    "        logits = policy(features)\n",
    "        alloc = softmax_allocation(logits)\n",
    "        \n",
    "        allocations['roots'].append(float(alloc.roots))\n",
    "        allocations['trunk'].append(float(alloc.trunk))\n",
    "        allocations['shoots'].append(float(alloc.shoots))\n",
    "        allocations['leaves'].append(float(alloc.leaves))\n",
    "        allocations['flowers'].append(float(alloc.flowers))\n",
    "        \n",
    "        # Step\n",
    "        state = step(state, alloc, light, moisture, wind, config, day)\n",
    "    \n",
    "    return allocations, states\n",
    "\n",
    "\n",
    "# Analyze trained policy\n",
    "allocs_mild, states_mild = analyze_policy_trajectory(policy_mild, config, *mild_env)\n",
    "\n",
    "# Also run baseline for comparison\n",
    "baseline_traj = run_season(config, mild_climate, baseline_policy)\n",
    "baseline_allocs = baseline_traj.get_allocation_arrays()\n",
    "baseline_states = baseline_traj.get_state_arrays()\n",
    "\n",
    "print(f\"Trained policy seeds: {seed_hist_mild[-1]:.3f}\")\n",
    "print(f\"Baseline policy seeds: {float(baseline_traj.seeds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot allocation strategies\n",
    "days = np.arange(config.num_days)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Trained policy allocation (stacked)\n",
    "ax = axes[0, 0]\n",
    "ax.stackplot(days,\n",
    "    allocs_mild['roots'], allocs_mild['trunk'], allocs_mild['shoots'],\n",
    "    allocs_mild['leaves'], allocs_mild['flowers'],\n",
    "    labels=['Roots', 'Trunk', 'Shoots', 'Leaves', 'Flowers'],\n",
    "    alpha=0.8)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Allocation')\n",
    "ax.set_title('Trained Neural Policy - Allocation Strategy')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Baseline allocation (stacked)\n",
    "ax = axes[0, 1]\n",
    "ax.stackplot(days,\n",
    "    baseline_allocs['roots'], baseline_allocs['trunk'], baseline_allocs['shoots'],\n",
    "    baseline_allocs['leaves'], baseline_allocs['flowers'],\n",
    "    labels=['Roots', 'Trunk', 'Shoots', 'Leaves', 'Flowers'],\n",
    "    alpha=0.8)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Allocation')\n",
    "ax.set_title('Baseline Policy - Allocation Strategy')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Biomass comparison\n",
    "ax = axes[1, 0]\n",
    "ax.plot(days, states_mild['leaves'], 'b-', label='Neural - Leaves', linewidth=2)\n",
    "ax.plot(days, states_mild['flowers'], 'r-', label='Neural - Flowers', linewidth=2)\n",
    "ax.plot(days, states_mild['trunk'], 'brown', label='Neural - Trunk', linewidth=2)\n",
    "ax.plot(np.arange(len(baseline_states['leaves'])), baseline_states['leaves'], 'b--', label='Baseline - Leaves', linewidth=2, alpha=0.7)\n",
    "ax.plot(np.arange(len(baseline_states['flowers'])), baseline_states['flowers'], 'r--', label='Baseline - Flowers', linewidth=2, alpha=0.7)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Biomass')\n",
    "ax.set_title('Biomass Trajectories')\n",
    "ax.legend()\n",
    "\n",
    "# Energy comparison\n",
    "ax = axes[1, 1]\n",
    "ax.plot(days, states_mild['energy'], 'g-', label='Neural', linewidth=2)\n",
    "ax.plot(np.arange(len(baseline_states['energy'])), baseline_states['energy'], 'g--', label='Baseline', linewidth=2, alpha=0.7)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Energy')\n",
    "ax.set_title('Energy Over Time')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(f'Neural Policy vs Baseline (Mild Climate)\\nNeural: {seed_hist_mild[-1]:.3f} seeds, Baseline: {float(baseline_traj.seeds):.3f} seeds', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Multi-Climate Training\n",
    "\n",
    "Now let's train a policy that works across multiple climates.\n",
    "The idea: average the loss over different environments so the policy learns robust strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_climate_loss(\n",
    "    policy: NeuralPolicy,\n",
    "    config: SimConfig,\n",
    "    environments: list[tuple[Array, Array, Array]],\n",
    "    l2_weight: float = 0.0001,\n",
    ") -> Array:\n",
    "    \"\"\"\n",
    "    Compute average loss across multiple climates.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    for light_arr, moisture_arr, wind_arr in environments:\n",
    "        _, seeds = rollout_neural(policy, config, light_arr, moisture_arr, wind_arr)\n",
    "        total_loss = total_loss - seeds\n",
    "    \n",
    "    # Average over climates\n",
    "    avg_loss = total_loss / len(environments)\n",
    "    \n",
    "    # L2 regularization\n",
    "    params = eqx.filter(policy, eqx.is_array)\n",
    "    l2 = sum(jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params))\n",
    "    \n",
    "    return avg_loss + l2_weight * l2\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def multi_climate_train_step(\n",
    "    policy: NeuralPolicy,\n",
    "    opt_state,\n",
    "    optimizer,\n",
    "    config: SimConfig,\n",
    "    environments: list[tuple[Array, Array, Array]],\n",
    "):\n",
    "    \"\"\"\n",
    "    Training step for multi-climate loss.\n",
    "    \"\"\"\n",
    "    loss, grads = eqx.filter_value_and_grad(multi_climate_loss)(\n",
    "        policy, config, environments\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, policy)\n",
    "    policy = eqx.apply_updates(policy, updates)\n",
    "    return policy, opt_state, loss\n",
    "\n",
    "\n",
    "def train_multi_climate(\n",
    "    policy: NeuralPolicy,\n",
    "    config: SimConfig,\n",
    "    environments: list[tuple[Array, Array, Array]],\n",
    "    num_steps: int = 400,\n",
    "    learning_rate: float = 0.01,\n",
    ") -> tuple[NeuralPolicy, list[float], dict[str, list[float]]]:\n",
    "    \"\"\"\n",
    "    Train neural policy on multiple climates.\n",
    "    \"\"\"\n",
    "    schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=learning_rate * 0.1,\n",
    "        peak_value=learning_rate,\n",
    "        warmup_steps=30,\n",
    "        decay_steps=num_steps,\n",
    "        end_value=learning_rate * 0.01,\n",
    "    )\n",
    "    optimizer = optax.adam(schedule)\n",
    "    opt_state = optimizer.init(eqx.filter(policy, eqx.is_array))\n",
    "    \n",
    "    loss_history = []\n",
    "    seed_histories = {i: [] for i in range(len(environments))}\n",
    "    \n",
    "    for step_i in tqdm(range(num_steps), desc=\"Multi-climate training\"):\n",
    "        policy, opt_state, loss = multi_climate_train_step(\n",
    "            policy, opt_state, optimizer, config, environments\n",
    "        )\n",
    "        \n",
    "        loss_history.append(float(loss))\n",
    "        \n",
    "        # Track seeds per climate\n",
    "        for i, env in enumerate(environments):\n",
    "            _, seeds = rollout_neural(policy, config, *env)\n",
    "            seed_histories[i].append(float(seeds))\n",
    "        \n",
    "        if step_i % 100 == 0:\n",
    "            seeds_str = \", \".join(f\"{seed_histories[i][-1]:.2f}\" for i in range(len(environments)))\n",
    "            print(f\"Step {step_i:3d}: loss={float(loss):.4f}, seeds=[{seeds_str}]\")\n",
    "    \n",
    "    return policy, loss_history, seed_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all three climates\n",
    "print(\"=\"*60)\n",
    "print(\"Training on MULTIPLE climates (mild, windy, droughty)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "policy_multi = NeuralPolicy(key, hidden_size=32, num_hidden=2)\n",
    "\n",
    "environments = [mild_env, windy_env, droughty_env]\n",
    "climate_names = ['Mild', 'Windy', 'Droughty']\n",
    "\n",
    "policy_multi, loss_hist_multi, seed_hists_multi = train_multi_climate(\n",
    "    policy_multi, config, environments,\n",
    "    num_steps=400,\n",
    "    learning_rate=0.01,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal seeds:\")\n",
    "for i, name in enumerate(climate_names):\n",
    "    print(f\"  {name}: {seed_hists_multi[i][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multi-climate training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(loss_hist_multi, linewidth=2)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_title('Multi-Climate Training Loss')\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:red']\n",
    "for i, (name, color) in enumerate(zip(climate_names, colors)):\n",
    "    ax2.plot(seed_hists_multi[i], label=name, color=color, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Seeds')\n",
    "ax2.set_title('Seed Production by Climate')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Compare All Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: baseline vs single-climate trained vs multi-climate trained\n",
    "\n",
    "def evaluate_neural_policy(policy, config, environments, climate_names):\n",
    "    \"\"\"Evaluate neural policy on each climate.\"\"\"\n",
    "    results = {}\n",
    "    for name, env in zip(climate_names, environments):\n",
    "        final_state, seeds = rollout_neural(policy, config, *env)\n",
    "        results[name] = {\n",
    "            'seeds': float(seeds),\n",
    "            'biomass': float(final_state.total_biomass()),\n",
    "            'flowers': float(final_state.flowers),\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Evaluate all policies\n",
    "climates_list = [mild_climate, windy_climate, droughty_climate]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POLICY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Policy':<25} {'Mild':>12} {'Windy':>12} {'Droughty':>12} {'Total':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Baseline\n",
    "baseline_seeds = []\n",
    "for climate in climates_list:\n",
    "    traj = run_season(config, climate, baseline_policy)\n",
    "    baseline_seeds.append(float(traj.seeds))\n",
    "print(f\"{'Baseline':<25} {baseline_seeds[0]:>12.3f} {baseline_seeds[1]:>12.3f} {baseline_seeds[2]:>12.3f} {sum(baseline_seeds):>12.3f}\")\n",
    "\n",
    "# Single-climate trained (mild)\n",
    "single_results = evaluate_neural_policy(policy_mild, config, environments, climate_names)\n",
    "single_seeds = [single_results[n]['seeds'] for n in climate_names]\n",
    "print(f\"{'Neural (mild-trained)':<25} {single_seeds[0]:>12.3f} {single_seeds[1]:>12.3f} {single_seeds[2]:>12.3f} {sum(single_seeds):>12.3f}\")\n",
    "\n",
    "# Multi-climate trained\n",
    "multi_results = evaluate_neural_policy(policy_multi, config, environments, climate_names)\n",
    "multi_seeds = [multi_results[n]['seeds'] for n in climate_names]\n",
    "print(f\"{'Neural (multi-trained)':<25} {multi_seeds[0]:>12.3f} {multi_seeds[1]:>12.3f} {multi_seeds[2]:>12.3f} {sum(multi_seeds):>12.3f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-climate policy strategies\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for col, (name, env) in enumerate(zip(climate_names, environments)):\n",
    "    allocs, states = analyze_policy_trajectory(policy_multi, config, *env)\n",
    "    \n",
    "    # Allocation\n",
    "    ax = axes[0, col]\n",
    "    ax.stackplot(days,\n",
    "        allocs['roots'], allocs['trunk'], allocs['shoots'],\n",
    "        allocs['leaves'], allocs['flowers'],\n",
    "        labels=['Roots', 'Trunk', 'Shoots', 'Leaves', 'Flowers'],\n",
    "        alpha=0.8)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel('Allocation')\n",
    "    ax.set_title(f'{name} Climate - Allocation')\n",
    "    ax.set_ylim(0, 1)\n",
    "    if col == 2:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Biomass\n",
    "    ax = axes[1, col]\n",
    "    ax.plot(days, states['leaves'], label='Leaves', linewidth=2)\n",
    "    ax.plot(days, states['trunk'], label='Trunk', linewidth=2)\n",
    "    ax.plot(days, states['flowers'], label='Flowers', linewidth=2)\n",
    "    ax.plot(days, states['roots'], label='Roots', linewidth=2, alpha=0.7)\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel('Biomass')\n",
    "    ax.set_title(f'{name} Climate - Biomass')\n",
    "    if col == 2:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.suptitle('Multi-Climate Trained Policy - Adaptation Across Environments', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NEURAL POLICY TRAINING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. SINGLE-CLIMATE TRAINING (Mild)\n",
    "   - Successfully learned to maximize seeds in mild conditions\n",
    "   - Discovered temporal strategy: leaves early, flowers late\n",
    "   - May overfit to mild conditions (poor generalization)\n",
    "\n",
    "2. MULTI-CLIMATE TRAINING\n",
    "   - Learns robust strategies that work across environments\n",
    "   - Trades off peak performance for consistent performance\n",
    "   - Adapts allocation based on current state AND environment\n",
    "\n",
    "3. KEY FINDINGS\n",
    "   - Neural policy outperforms baseline (hand-coded) in trained climates\n",
    "   - State-awareness allows dynamic adaptation (e.g., respond to low energy)\n",
    "   - Environment-awareness enables climate-specific strategies\n",
    "\n",
    "4. WHAT THE POLICY LEARNED\n",
    "   - Early season: Heavy leaf investment for photosynthesis\n",
    "   - Mid season: Balanced growth with increasing trunk\n",
    "   - Late season: Rapid shift to flowers for reproduction\n",
    "   - Under stress: More conservative, defensive allocations\n",
    "\n",
    "5. NEXT STEPS\n",
    "   - Curriculum learning (start easy, increase difficulty)\n",
    "   - Larger networks for more complex strategies\n",
    "   - Visualize learned representations\n",
    "   - Test on unseen climate combinations\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Save Trained Policies (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Save policies\n",
    "save_dir = Path(\"../trained_policies\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save using equinox serialization\n",
    "eqx.tree_serialise_leaves(save_dir / \"policy_mild.eqx\", policy_mild)\n",
    "eqx.tree_serialise_leaves(save_dir / \"policy_multi.eqx\", policy_multi)\n",
    "\n",
    "print(f\"Saved policies to {save_dir}\")\n",
    "\n",
    "# To load later:\n",
    "# policy_loaded = eqx.tree_deserialise_leaves(save_dir / \"policy_mild.eqx\", policy_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
